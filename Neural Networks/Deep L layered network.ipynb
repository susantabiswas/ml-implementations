{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Deep L-layered Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will make a L -layers deep neural network for cat classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cat classification problem, where the task is to classify a picture as a cat or non-cat picture.\n",
    "<br>Output y = 1 if the picture is of a cat \n",
    "<br>Output y = 0 if the picture is not of a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import dataset_utility as du\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains cat and non-cat pictures and the labels are 0 or 1.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:(209, 64, 64, 3)\n",
      "Training output shape:(209,)\n",
      "Test features shape:(50, 64, 64, 3)\n",
      "Test output shape:(50,)\n"
     ]
    }
   ],
   "source": [
    "# load the data set\n",
    "test_loc = r'datasets\\train_catvsnoncat.h5'\n",
    "train_loc = r'datasets\\test_catvsnoncat.h5'\n",
    "\n",
    "train_X, train_y, test_X, test_y = du.load_data2(train_loc, test_loc)\n",
    "\n",
    "print('Training features shape:'+ str(train_X.shape))\n",
    "print('Training output shape:'+ str(train_y.shape))\n",
    "print('Test features shape:'+ str(test_X.shape))\n",
    "print('Test output shape:'+ str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) <u>Reshape the matrices.</u>\n",
    "### 2) <u>Flatten the feature matrix and create one hot matrix for output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the output matrices to avoid it being treated as rank 1 matrix\n",
    "train_y = np.reshape(train_y, (train_y.shape[0],1))\n",
    "test_y = np.reshape(test_y, (test_y.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noramalize the features\n",
    "train_X, test_X = du.normalize_data( train_X, test_X)\n",
    "\n",
    "# flatten the feature matrix\n",
    "train_X, test_X = du.unroll_features(train_X, test_X)\n",
    "\n",
    "# transpose the label matrices to match the dimension format\n",
    "train_y = train_y.T\n",
    "test_y = test_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:(12288, 209)\n",
      "Training output shape:(1, 209)\n",
      "Test features shape:(12288, 50)\n",
      "Test output shape:(1, 50)\n"
     ]
    }
   ],
   "source": [
    "# check the dimensions\n",
    "print('Training features shape:'+ str(train_X.shape))\n",
    "print('Training output shape:'+ str(train_y.shape))\n",
    "print('Test features shape:'+ str(test_X.shape))\n",
    "print('Test output shape:'+ str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Neural Network Architecture\n",
    "We will be using sigmoid activation function for the output layer and use a non-linear activation function like sigmoid, tanh, ReLU for the hidden layer units.\n",
    "<br>For each hidden layer [l]: <br>Activation output from[l-1] ---> Z[l] ----> Activation function----->A[l] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weight and bias parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_init_parameters(num_layers, n_layer):\n",
    "    '''\n",
    "    for initializing the weight and bias parameters with random values\n",
    "    \n",
    "    Arguments:\n",
    "        num_layers: (int) no. of layers in the neural network\n",
    "        n_layer: (list) no. of units in each layer\n",
    "    Returns:\n",
    "        parameters: (dict) zero initialized weight and bias parameters \n",
    "    '''\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(num_layers):\n",
    "        parameters['W' + str(l+1)] = np.random.randn(n_layer[l+1], n_layer[l]) * 0.01\n",
    "        parameters['b' + str(l+1)] = np.zeros((n_layer[l+1],1)) \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for doing the sigmoid function\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    for computing the sigmoid function\n",
    "    \n",
    "    Arguments:\n",
    "        z: linear input\n",
    "    Returns:\n",
    "        A: (float) sigmoid output\n",
    "    '''\n",
    "    A = (1/ (1 + np.exp(-z)))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for doing the ReLU function\n",
    "def relu(z):\n",
    "    '''\n",
    "    for computing the relu function\n",
    "    \n",
    "    Arguments:\n",
    "        z: linear input\n",
    "    Returns:\n",
    "        A: (float) relu output\n",
    "    '''\n",
    "    A = np.maximum(0, z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_func(Z, acti_func):\n",
    "    '''\n",
    "    for applying the activation function\n",
    "    \n",
    "    Arguments:\n",
    "        z: (numpy matrix) linear input\n",
    "        acti_func:(str) activation function name(choice)\n",
    "    Returns:\n",
    "        A: (float) non-linear activation output\n",
    "    '''\n",
    "    # apply the activation function based on the choice\n",
    "    if acti_func == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif acti_func == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif acti_func == 'tanh':\n",
    "        A = np.tanh(Z)\n",
    "    else:\n",
    "        A = relu(Z)\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for computing the logistic error\n",
    "def compute_cost(train_y, y_hat):\n",
    "    '''\n",
    "    for computing the mean loss (cost) for logistic regression\n",
    "    \n",
    "    Arguments:\n",
    "        train_y:(numpy matrix) contains the correct output labels for m training examples\n",
    "        y_hat:(numpy matrix) contains the sigmoid output values for m training examples\n",
    "    Returns:\n",
    "        cost: (float) cost value for m training examples\n",
    "    '''\n",
    "    cost = (1/train_y.shape[1]) * np.sum( -train_y*np.log(y_hat) - (1-train_y)*np.log(1 - y_hat) )\n",
    "    \n",
    "    # for avoiding 1 dimensional values \n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, grads, learning_rate, num_layers):\n",
    "    '''\n",
    "    for doing gradient descent and updating the weights\n",
    "    \n",
    "    Arguments:\n",
    "        parameters: (dict) contains the learned weight and bias parameters\n",
    "        grads:(dict) contains the gradients for different layers\n",
    "        learning_rate: (float) learning rate\n",
    "        num_layers:(int) no. of layers in the Neural network\n",
    "    Returns:\n",
    "        parameters: (dict) contains the learned weight and bias parameters\n",
    "    '''\n",
    "    # update the parameters\n",
    "    for l in range(num_layers):\n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - (learning_rate * grads['dW'+str(l+1)])\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - (learning_rate * grads['db'+str(l+1)])\n",
    "   \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(features_X, parameters, num_layers, acti_fun):\n",
    "    '''\n",
    "    for doing forward propagation in neural network\n",
    "    \n",
    "    Arguments:\n",
    "        features_X: (numpy matrix) features for all training examples\n",
    "        parameters: (dict) contains the learned weight and bias parameters\n",
    "        num_layers:(int) no. of layers in the Neural network\n",
    "        acti_fun: (str) name of the activation function to use\n",
    "    Returns:\n",
    "        inter_val: (dict) contains the linear output Z and activation output A for each layer\n",
    "    '''\n",
    "    # for saving the computed intermediate values for each layer\n",
    "    inter_val = {}\n",
    "    \n",
    "    # make activation for input layer equal to the features\n",
    "    A = features_X\n",
    "    inter_val['A0'] = A\n",
    "    \n",
    "    # do forward propagation for all hidden layers except for the output layer\n",
    "    for l in range(num_layers-1):\n",
    "        # first find the linear output for each layer\n",
    "        inter_val['Z' + str(l+1)] = np.dot(parameters['W' + str(l+1)], A) + parameters['b' + str(l+1)]\n",
    "        inter_val['A' + str(l+1)] = activation_func(inter_val['Z' + str(l+1)], acti_fun)\n",
    "        A = inter_val['A'+str(l+1)]\n",
    "        \n",
    "    # forward propagation for output layer\n",
    "    inter_val['Z' + str(num_layers)] = np.dot(parameters['W' + str(num_layers)], A) + parameters['b' + str(num_layers)]\n",
    "    inter_val['A' + str(num_layers)] = activation_func(inter_val['Z' + str(num_layers)], 'sigmoid')\n",
    "    \n",
    "    return inter_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_derivative(dA, Z, acti_func):\n",
    "    '''\n",
    "    for finding the derivative of linear output Z with a particular activation function in its layer\n",
    "    \n",
    "    Arguments:\n",
    "        dA: (numpy matrix) partial derivative of loss function wrt activation outputs for the given layer  \n",
    "        Z: (numpy matrix) contains the linear otuput for the given layer\n",
    "        acti_fun: (str) name of the activation function to use\n",
    "    Returns:\n",
    "        dZ: (numpy matrix) partial derivative of loss function wrt linear output Z for the given layer\n",
    "    '''\n",
    "    # for sigmoid activation function\n",
    "    if acti_func == 'sigmoid':\n",
    "        s = sigmoid(Z)\n",
    "        dZ = dA*s*(1-s)\n",
    "        \n",
    "    # for relu activation function\n",
    "    elif acti_func == 'relu':\n",
    "        dZ = np.copy(dA)\n",
    "        dZ[Z<=0] = 0\n",
    "        \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagate(train_X, train_y, parameters, inter_val, num_layers):\n",
    "    '''\n",
    "    for doing backward propagation in neural network\n",
    "    \n",
    "    Arguments:\n",
    "        train_X: (numpy matrix) features for all training examples\n",
    "        train_y: (numpy matrix) correct labels for all training examples\n",
    "        parameters: (dict) contains the learned weight and bias parameters\n",
    "        num_layers:(int) no. of layers in the Neural network\n",
    "        inter_val: (dict) contains the linear output Z and activation output A for each layer\n",
    "    Returns:\n",
    "        grads: (dict) contains the gradients for different layers\n",
    "    '''\n",
    "    #no. of training examples\n",
    "    m = train_X.shape[1]\n",
    "    \n",
    "    # for storing the gradients\n",
    "    grads = {}\n",
    "    \n",
    "    # gradients for output layer\n",
    "    AL = inter_val['A' + str(num_layers)]\n",
    "    dA = -train_y/AL + (1-train_y)/(1-AL)\n",
    "    dZ = activation_derivative(dA, inter_val['Z' + str(num_layers)], 'sigmoid')\n",
    "    grads['dW'+str(num_layers)] = (1/m)*np.dot(dZ, inter_val['A'+str(num_layers-1)].T)\n",
    "    grads['db'+str(num_layers)] = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA = np.dot(parameters['W'+str(num_layers)].T, dZ)\n",
    "    \n",
    "    for l in reversed(range(1,num_layers)):\n",
    "        dZ = activation_derivative(dA, inter_val['Z' + str(l)], 'relu')\n",
    "        grads['dW'+str(l)] = (1/m)*np.dot(dZ, inter_val['A'+str(l-1)].T)\n",
    "        grads['db'+str(l)] = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = np.dot(parameters['W'+str(l)].T, dZ)\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(train_X, train_y, num_layers, n_layer, num_iters, learning_rate):\n",
    "    '''\n",
    "    for training the model by doing steps systematically\n",
    "    \n",
    "    Arguments:\n",
    "        train_X:(numpy matrix) contains the input features for m training examples\n",
    "        train_y:(numpy matrix) contains the correct output labels for m training examples\n",
    "        num_iters: (int) no. of iterations to run \n",
    "        learning_rate: (float) learning rate\n",
    "     Returns:\n",
    "        parameters: (dict) contains the learned weight and bias parameters\n",
    "        costs: (list) contains cost per 100 iterations\n",
    "    '''\n",
    "    \n",
    "    parameters = random_init_parameters(num_layers, n_layer)\n",
    "    \n",
    "    # for storing the costs\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        # do forward propagation\n",
    "        inter_val = forward_propagation(train_X, parameters, num_layers, 'relu')\n",
    "        # compute the cost\n",
    "        cost = compute_cost(train_y, inter_val['A'+str(num_layers)])\n",
    "        # do backward propagation\n",
    "        grads = backward_propagate(train_X, train_y, parameters, inter_val, num_layers)\n",
    "        # update parameters\n",
    "        parameters = gradient_descent(parameters, grads, learning_rate, num_layers)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('Cost after iteration %i:%f'%(i,cost))\n",
    "        costs.append(cost)    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0:0.693127\n",
      "Cost after iteration 100:0.647598\n",
      "Cost after iteration 200:0.634713\n",
      "Cost after iteration 300:0.606501\n",
      "Cost after iteration 400:0.563252\n",
      "Cost after iteration 500:0.512054\n",
      "Cost after iteration 600:0.451598\n",
      "Cost after iteration 700:0.388893\n",
      "Cost after iteration 800:0.363807\n",
      "Cost after iteration 900:0.364767\n",
      "Cost after iteration 1000:0.334600\n",
      "Cost after iteration 1100:0.299498\n",
      "Cost after iteration 1200:0.220528\n",
      "Cost after iteration 1300:0.158900\n",
      "Cost after iteration 1400:0.131092\n",
      "Cost after iteration 1500:0.110172\n",
      "Cost after iteration 1600:0.094221\n",
      "Cost after iteration 1700:0.080147\n",
      "Cost after iteration 1800:0.068123\n",
      "Cost after iteration 1900:0.058971\n",
      "Cost after iteration 2000:0.051670\n",
      "Cost after iteration 2100:0.045665\n",
      "Cost after iteration 2200:0.040592\n",
      "Cost after iteration 2300:0.036457\n",
      "Cost after iteration 2400:0.032860\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "n_layer = [12288, 7, 1]\n",
    "learning_rate = 0.0075\n",
    "num_iters = 2500\n",
    "parameters, costs = nn_model(train_X, train_y, num_layers, n_layer, num_iters, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(costs, learning_rate):\n",
    "    '''\n",
    "    for plotting learning curve\n",
    "    \n",
    "    Arguments:\n",
    "        costs: (list) contains the costs per 100 iterations\n",
    "        learning_rate: (float) learning rate\n",
    "    Returns:\n",
    "        nil\n",
    "    '''\n",
    "    # Plot learning curve( cost vs iterations)\n",
    "    costs = np.squeeze(costs)\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24bf808aef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(costs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(features_X, parameters):\n",
    "    '''\n",
    "    for predicting the output labels for inputs\n",
    "    \n",
    "    Arguments:\n",
    "        feature_X: (numpy matrix) contains the input features for m training examples\n",
    "        parameters: (dict) contains the learned weight and bias parameters\n",
    "    Returns:\n",
    "        pred: (numpy array) output labels\n",
    "    '''\n",
    "    # find the output of logistic regression\n",
    "    inter_val = forward_propagation(features_X, parameters, num_layers, 'relu')\n",
    "    # take the sigmoid output\n",
    "    pred = inter_val['A'+str(num_layers)]\n",
    "    \n",
    "    # if the output is >0.5 then we consider that the picture is of a cat else it is a non-cat picture. \n",
    "    pred = (pred > 0.5)\n",
    "    \n",
    "    return pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 100.0 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "prediction_train = predict(train_X, parameters)\n",
    "prediction_test = predict(test_X, parameters)\n",
    "\n",
    "# Print train/test Errors\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(prediction_train - train_y)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(prediction_test - test_y)) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
